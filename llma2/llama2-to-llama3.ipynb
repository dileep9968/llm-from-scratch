{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98d8501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobfile version: 3.0.0\n",
      "huggingface_hub version: 0.31.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    'blobfile',         # to download pretrained weights\n",
    "    'huggingface_hub',  # to download pretrained weights\n",
    "    'tiktoken',         # to implement the tokenizer\n",
    "    'torch'\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1df97f",
   "metadata": {},
   "source": [
    "### 1. Convert the llama model implementation step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9915ac",
   "metadata": {},
   "source": [
    "#### 1.1 Resuing Llama 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf037df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "import nbformat\n",
    "import types\n",
    "\n",
    "def import_from_notebook():\n",
    "    def import_definitions_from_notebook(fullname, names):\n",
    "        current_dir = os.getcwd()\n",
    "        path = os.path.join(current_dir, fullname+'.ipynb')\n",
    "        path = os.path.normpath(path)\n",
    "\n",
    "        # Load the notebook\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Notebook file not found at: {path}\")\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version = 4)\n",
    "        \n",
    "        # create a module to store the imported functions and classes\n",
    "        mod = types.ModuleType(fullname)\n",
    "        sys.modules[fullname]=mod\n",
    "\n",
    "        # Go through the notebook cells and only execuite functions and class definition\n",
    "        for cell in nb.cells:\n",
    "            if cell.cell_type =='code':\n",
    "                cell_code = cell.source\n",
    "                for name in names:\n",
    "                    # check for function or class definitions\n",
    "                    if f\"def {name}\" in cell_code or f'class {name}' in cell_code:\n",
    "                        exec(cell_code, mod.__dict__)\n",
    "        return mod\n",
    "    \n",
    "    fullname = 'gpt-to-llama2'\n",
    "    names = ['precompute_rope_params', 'compute_rope', 'SiLU', \"FeedForward\", 'MultiHeadAttention']\n",
    "    return import_definitions_from_notebook(fullname, names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0898dc1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Notebook file not found at: c:\\Users\\hp\\OneDrive\\Desktop\\llm-from-scratch\\llma2\\gpt-to-llama2.ipynb",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m imported_module = \u001b[43mimport_from_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# We need to redefine precompute_rope_params \u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# precompute_rope_params = getattr(imported_module, 'precompute_rope_params', None)\u001b[39;00m\n\u001b[32m      5\u001b[39m compute_rope = \u001b[38;5;28mgetattr\u001b[39m(imported_module, \u001b[33m'\u001b[39m\u001b[33mcompute_rope\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mimport_from_notebook\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m fullname = \u001b[33m'\u001b[39m\u001b[33mgpt-to-llama2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     34\u001b[39m names = [\u001b[33m'\u001b[39m\u001b[33mprecompute_rope_params\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcompute_rope\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSiLU\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFeedForward\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMultiHeadAttention\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimport_definitions_from_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mimport_from_notebook.<locals>.import_definitions_from_notebook\u001b[39m\u001b[34m(fullname, names)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load the notebook\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(path):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNotebook file not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m io.open(path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     17\u001b[39m     nb = nbformat.read(f, as_version = \u001b[32m4\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Notebook file not found at: c:\\Users\\hp\\OneDrive\\Desktop\\llm-from-scratch\\llma2\\gpt-to-llama2.ipynb"
     ]
    }
   ],
   "source": [
    "imported_module = import_from_notebook()\n",
    "\n",
    "# We need to redefine precompute_rope_params \n",
    "# precompute_rope_params = getattr(imported_module, 'precompute_rope_params', None)\n",
    "compute_rope = getattr(imported_module, 'compute_rope', None)\n",
    "SiLU = getattr(import_from_notebook, 'SiLU', None)\n",
    "FeedForward = getattr(imported_module, 'FeedForward', None)\n",
    "RMSNorm = getattr(imported_module, 'RMSNorm', None)\n",
    "\n",
    "# MultiheadAttention only for comparsion purpose\n",
    "MultiHeadAttention = getattr(imported_module, 'MultiHeadAttention', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2ec91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d310b72c",
   "metadata": {},
   "source": [
    "#### 1.2 Modified RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d189cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f045e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096,freq_config=None ):\n",
    "    assert head_dim %2==0, \"Embeddings dimension must be even\"\n",
    "\n",
    "    # compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** ( torch.arange(0, head_dim, 2)[: (head_dim//2)].float()/head_dim))\n",
    "\n",
    "    # frequency adjustment\n",
    "    if freq_config is not None:\n",
    "        low_freq_wavelen = freq_config['original_context_length']/ freq_config['low_freq_factor']\n",
    "        high_freq_wavelen = freq_config['original_context_length']/ freq_config['high_freq_factor']\n",
    "\n",
    "        wavelen = 2* torch.pi / inv_freq\n",
    "\n",
    "        inv_freq_llama = torch.where(\n",
    "            wavelen > low_freq_wavelen, inv_freq / freq_config['factor'], inv_freq\n",
    "        )\n",
    "\n",
    "        smooth_factor = (freq_config['origianl_context_length']/ wavelen - freq_config['low_freq_factor'])/(\n",
    "                        freq_config['high_freq_factor']-freq_config['low_freq_factor'])\n",
    "\n",
    "        smoothed_inv_freq = (\n",
    "            (1- smooth_factor)*(inv_freq/ freq_config['factor']) + smooth_factor * inv_freq\n",
    "        )\n",
    "\n",
    "        is_medium_freq = (wavelen <= low_freq_wavelen) & ( wavelen >= high_freq_wavelen)\n",
    "        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "        inv_freq = inv_freq_llama\n",
    "\n",
    "    \n",
    "    #### Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    #compute the angle\n",
    "    analges = positions[:, None] *  inv_freq[None, :] # Shape: (context_length, head_dim//2)\n",
    "\n",
    "    # Expand angles to match head_dim\n",
    "    analges = torch.cat([analges, analges], dim=1) # Shape: (context_lenght, head_dim)\n",
    "\n",
    "    #Precompute sine and cosine\n",
    "    cos = torch.cos(analges)\n",
    "    sin = torch.sin(analges)\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd141570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate RoPE parameters\n",
    "\n",
    "llama_2_context_len = 4096\n",
    "llama_3_context_len = 8192\n",
    "\n",
    "llama_2_theta_base = 10_000\n",
    "llama_3_theta_base = 500_000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "938eb148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size =2\n",
    "num_heads=4\n",
    "head_dim =16\n",
    "\n",
    "# Instantiate RoPE parameters\n",
    "cos, sin = precompute_rope_params(\n",
    "    head_dim=head_dim,\n",
    "    theta_base=llama_3_theta_base,\n",
    "    context_length=llama_3_context_len\n",
    ")\n",
    "\n",
    "# Dummy query and key tensors\n",
    "torch.manual_seed(123)\n",
    "queries = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
    "\n",
    "# Apply rotary position embedding\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "keys_rot = compute_rope(keys, cos, sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69fd16b",
   "metadata": {},
   "source": [
    "#### 1.3 Grouped-query attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22e03758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class SharedBuffers:\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype = torch.float32):\n",
    "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
    "\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            #Create or fetch the buffers\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        return SharedBuffers._buffers[key]\n",
    "    \n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_in,\n",
    "            d_out,\n",
    "            context_length,\n",
    "            num_heads,\n",
    "            num_kv_groups,\n",
    "            rope_base=10_000,\n",
    "            rope_config=None,\n",
    "            dtype = None):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads ==0, 'd_out must be divisible by num_heads'\n",
    "        assert num_heads % num_kv_groups ==0, 'num_heads must be divisiable by num_kv_gropus'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)   \n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)  \n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads//num_kv_groups\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out , bias=False,dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)  \n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "        self.register_buffer('cos', cos)\n",
    "        self.register_buffer('sin', sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Reshape queries keys and values\n",
    "        queries = queries.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(values, self.cos, self.sin)\n",
    "\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        assert keys.shape[-1]==self.head_dim\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5891229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
